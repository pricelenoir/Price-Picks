{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc2be87-fbe3-430b-82ef-94b9c99210ee",
   "metadata": {},
   "source": [
    "This script will be run each week to update the database with the prior week's data and before making predictions for the upcoming week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b499c68d-c60f-4fb2-b53b-07d579398533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "import time\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81f03a29-b57e-4f99-aa4b-6a15805857bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set current date and determine how many days to filter the run\n",
    "season = 2023\n",
    "filter_days = 104\n",
    "\n",
    "# Filter files based on the start date\n",
    "current_date = datetime.today().strftime(\"%Y%m%d\") + \"0\"\n",
    "start_date = (datetime.today() - timedelta(days=filter_days)).strftime(\"%Y%m%d\") + \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb440491-675e-4c47-a880-bb21cf66483f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_date = '202312160'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c7cc84-f5ef-496b-b29a-a2fe00b2e473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory named data with 2 subdirectories: standings and scores\n",
    "DATA_DIR = \"data\"\n",
    "STANDINGS_DIR = os.path.join(DATA_DIR, \"standings\")\n",
    "SCORES_DIR = os.path.join(DATA_DIR, \"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37caddb0-f01f-481f-8e28-a644d18891c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standings_files = os.listdir(STANDINGS_DIR)\n",
    "standings_files = [s for s in standings_files if \".htm\" in s]\n",
    "\n",
    "box_scores = os.listdir(SCORES_DIR)\n",
    "box_scores = [os.path.join(SCORES_DIR, f) for f in box_scores if f.endswith(\".htm\") and f[:8] >= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a49bd65-13dd-48d0-8cf5-37c34e5e3a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def get_html(url, selector, sleep = 5, retries = 3):\n",
    "    # Selector will be a CSS selector -- ID that's used to locate unique element within the html\n",
    "    html = None\n",
    "    # Allows for more time to avoid sending to many requests and getting banned\n",
    "    for i in range(1, retries+1):\n",
    "        time.sleep(sleep * i)\n",
    "        # Logic to handle errors when web scraping\n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                browser = await p.firefox.launch() # Can also use p.chromium.launch() if issues persist\n",
    "                page = await browser.new_page()\n",
    "                await page.goto(url)\n",
    "                print(await page.title())\n",
    "                html = await page.inner_html(selector)\n",
    "        except PlaywrightTimeout:\n",
    "            print(f\"Timeout error on {url}\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f683ca2-19ef-48b0-af71-2f08b86dbd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_season(season):\n",
    "    url = f\"https://www.pro-football-reference.com/years/{season}/games.htm\"\n",
    "    html = await get_html(url, \"#all_games\")\n",
    "    \n",
    "    save_path = os.path.join(STANDINGS_DIR, f\"{season}-games.htm\") # Designate the filename and path\n",
    "            \n",
    "    with open(save_path, \"w+\") as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc4ec30-ee22-49d3-91c2-911f1f12cf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 NFL Weekly League Schedule | Pro-Football-Reference.com\n"
     ]
    }
   ],
   "source": [
    "await scrape_season(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c70fed-326b-4dae-8e28-3f1595f81086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_game(standings_file, start_date, current_date):\n",
    "    with open(standings_file, 'r') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    links = soup.find_all(\"a\") # Finds all \"a\" (anchor) tags\n",
    "    hrefs = [l.get(\"href\") for l in links] # Grab href part of the anchor tag\n",
    "\n",
    "    # Check for non-empty tag with \"boxscore\" and \".htm\" and filter based on start_date\n",
    "    box_scores = [l for l in hrefs if l and \"boxscore\" in l and \".htm\" in l]\n",
    "    box_scores = [url for url in box_scores if start_date <= url[11:19] <= current_date]\n",
    "    box_scores = [f\"https://pro-football-reference.com{l}\" for l in box_scores]\n",
    "\n",
    "    for url in box_scores:\n",
    "        save_path = os.path.join(SCORES_DIR, url.split(\"/\")[-1])\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        html = await get_html(url, \"#content\")\n",
    "        if not html:\n",
    "            continue\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825d145c-f338-4542-bac2-5a45a1b503a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop to iterate through standings directory and scrape each individual box score page\n",
    "for f in standings_files:\n",
    "    filepath = os.path.join(STANDINGS_DIR, f)\n",
    "    await scrape_game(filepath, start_date, current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e3486-ae7d-4a81-ac6c-7a68b3e0ea82",
   "metadata": {},
   "source": [
    "New games have been scraped. Now, parse the each boxscore and compile into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b2259c-c858-4aa9-8d14-11ff178cd748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_html(box_score):\n",
    "    with open(box_score) as f:\n",
    "        html = f.read() # Open and read html file\n",
    "        \n",
    "    soup = BeautifulSoup(html) # Use BeautifulSoup to parse html\n",
    "    # Parses and removes elements and labels from the box score table\n",
    "    [s.decompose() for s in soup.select(\"tr.over_header\")] # Select tr (table row) tag with class \"over_header\"\n",
    "    [s.decompose() for s in soup.select(\"tr.thead\")] # Then calls decompose on both to remove them from HTML\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1aa3e18-9972-4242-973d-5b84258da4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_game_info(soup):\n",
    "    #Get final score:\n",
    "    score = pd.read_html(str(soup), attrs={\"id\": \"scoring\"})[0]\n",
    "    score_df = pd.DataFrame(columns=[\"Tm\", \"Score\"])\n",
    "    \n",
    "    # Extract final score\n",
    "    score_df[\"Score\"] = score.iloc[-1, -2:].tolist()\n",
    "\n",
    "    # Get team stats:\n",
    "    team_stats = pd.read_html(str(soup), attrs={\"id\": \"team_stats\"})[0]\n",
    "\n",
    "    column_names = [\"Tm\", \"FrDwns\", \"TotYds\", \"TO\", \"Pen\", \"Pen_Yds\", \"ThdDwns\", \"ThdDwnAtt\"]\n",
    "    stats_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    # Pull each individual stat:\n",
    "    stats_df[\"Tm\"] = team_stats.columns[1:].tolist()\n",
    "    stats_df[\"FrDwns\"] = team_stats.iloc[0,-2:].tolist()\n",
    "    stats_df[\"TotYds\"] = team_stats.iloc[5,-2:].tolist()\n",
    "    stats_df[\"TO\"] = team_stats.iloc[7,-2:].tolist()\n",
    "\n",
    "    # Correct format penalty and third down values:\n",
    "    pen_values = team_stats.iloc[8, -2:].apply(lambda x: x.split('-'))\n",
    "    stats_df[\"Pen\"] = [val[0] for val in pen_values]\n",
    "    stats_df[\"Pen_Yds\"] = [val[1] for val in pen_values]\n",
    "\n",
    "    thddwn_values = team_stats.iloc[9, -2:].apply(lambda x: x.split('-'))\n",
    "    stats_df[\"ThdDwns\"] = [val[0] for val in thddwn_values]\n",
    "    stats_df[\"ThdDwnAtt\"] = [val[1] for val in thddwn_values]\n",
    "\n",
    "    score_df[\"Tm\"] = stats_df[\"Tm\"]\n",
    "    \n",
    "    game_info = score_df.merge(stats_df, on=\"Tm\")\n",
    "    return game_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301b30a5-8b94-4076-a083-d4ef24072341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_passing(soup):\n",
    "    passing_stats = pd.read_html(str(soup), attrs={\"id\": \"player_offense\"})[0]\n",
    "    advanced_stats = pd.read_html(str(soup), attrs={\"id\": \"passing_advanced\"})[0]\n",
    "\n",
    "    # Clean data: concatenate rows and remove unneeded columns\n",
    "    columns_to_keep = [\"Tm\", \"Cmp\", \"Att\", \"Yds\", \"TD\", \"Int\", \"Rate\"]\n",
    "    passing_stats = passing_stats[columns_to_keep]\n",
    "\n",
    "    # Calculate QB rating for the team\n",
    "    team1 = passing_stats.iloc[0][\"Tm\"]\n",
    "    team2 = passing_stats.iloc[-1][\"Tm\"]\n",
    "    team1_qbr = round((passing_stats.loc[passing_stats[\"Tm\"] == team1, \"Rate\"] * passing_stats.loc[passing_stats[\"Tm\"] == team1, \"Att\"]).sum() / passing_stats.loc[passing_stats[\"Tm\"] == team1, \"Att\"].sum(), 1)\n",
    "    team2_qbr = round((passing_stats.loc[passing_stats[\"Tm\"] == team2, \"Rate\"] * passing_stats.loc[passing_stats[\"Tm\"] == team2, \"Att\"]).sum() / passing_stats.loc[passing_stats[\"Tm\"] == team2, \"Att\"].sum(), 1)\n",
    "\n",
    "    # Concatenate: Add up values per team and add the overall passing rate back in\n",
    "    passing_stats = passing_stats.groupby(\"Tm\").sum().reset_index()\n",
    "\n",
    "    # Clean advanced stats\n",
    "    columns_to_keep = [\"Tm\", \"YAC\", \"Drops\", \"BadTh\", \"Sk\", \"Hrry\", \"Hits\", \"Prss\"]\n",
    "    advanced_stats = advanced_stats[columns_to_keep]\n",
    "\n",
    "    # Concatenate: Add up values and add calculated values back to dataframe\n",
    "    advanced_stats = advanced_stats.groupby(\"Tm\").sum().reset_index()\n",
    "\n",
    "    # Concatenate both data frames\n",
    "    passing_stats = passing_stats.merge(advanced_stats, on=\"Tm\")\n",
    "\n",
    "    # Rename specific columns with \"Pass_\" prefix\n",
    "    columns_to_rename = [\"Cmp\", \"Att\", \"Yds\", \"TD\", \"Int\"]\n",
    "    passing_stats = passing_stats.rename(columns=lambda x: \"Pass_\" + x if x in columns_to_rename else x)\n",
    "\n",
    "    # Rename specific columns with \"QB_\" prefix\n",
    "    columns_to_rename = [\"Rate\", \"BadTh\", \"Sk\", \"Hrry\", \"Hits\", \"Prss\"]\n",
    "    passing_stats = passing_stats.rename(columns=lambda x: \"QB_\" + x if x in columns_to_rename else x)\n",
    "\n",
    "    # Add team QB rating columns\n",
    "    passing_stats.loc[passing_stats[\"Tm\"] == team1, \"QB_Rate\"] = team1_qbr\n",
    "    passing_stats.loc[passing_stats[\"Tm\"] == team2, \"QB_Rate\"] = team2_qbr\n",
    "    \n",
    "    return passing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0532aea-008c-40db-8440-8cabbfd67fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rushing(soup):\n",
    "    rushing_stats = pd.read_html(str(soup), attrs={\"id\": \"rushing_advanced\"})[0]\n",
    "\n",
    "    # Remove unneeded columns\n",
    "    columns_to_keep = [\"Tm\", \"Att\", \"Yds\", \"TD\", \"YAC\"]\n",
    "    rushing_stats = rushing_stats[columns_to_keep]\n",
    "\n",
    "    # Concatenate: Add up values per team\n",
    "    rushing_stats = rushing_stats.groupby(\"Tm\").sum().reset_index()\n",
    "\n",
    "    # Calculate yards per carry (YPC)\n",
    "    rushing_stats[\"YPC\"] = (rushing_stats[\"Yds\"] / rushing_stats[\"Att\"]).round(1)\n",
    "\n",
    "     # Rename column labels with \"Rush_\" prefix\n",
    "    rushing_stats = rushing_stats.rename(columns=lambda x: \"Rush_\" + x if x != \"Tm\" else x)\n",
    "    \n",
    "    return rushing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c36fdef-1150-4701-8edf-d6000ae3edab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kicking(soup):\n",
    "    kicking_stats = pd.read_html(str(soup), attrs={\"id\": \"kicking\"})[0]\n",
    "\n",
    "    columns_to_keep = [\"Tm\", \"XPM\", \"XPA\", \"FGM\", \"FGA\"]\n",
    "    kicking_stats = kicking_stats[columns_to_keep]\n",
    "    \n",
    "    kicking_stats = kicking_stats.groupby(\"Tm\").sum().reset_index()\n",
    "    \n",
    "    return kicking_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9206fbb4-a0ba-4f6d-8a31-95e50f26096a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to determine which season a particular game was played\n",
    "def read_season_info(soup):\n",
    "    nav = soup.select(\"#bottom_nav\")[0]\n",
    "    hrefs = [a[\"href\"] for a in nav.find_all('a')]\n",
    "    season = hrefs[2].split(\"/\")[2]\n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d8a4d3-23e1-415f-8ec7-46ec4ad4fbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataframe(soup, box_score):\n",
    "    # Concatenate and build the dataframe\n",
    "    game_info = get_game_info(soup)\n",
    "    passing = get_passing(soup)\n",
    "    rushing = get_rushing(soup)\n",
    "    kicking = get_kicking(soup)\n",
    "\n",
    "    dataframe = game_info.merge(passing, on=\"Tm\").merge(rushing, on=\"Tm\").merge(kicking, on=\"Tm\")\n",
    "    \n",
    "    game_opp = dataframe.iloc[::-1].reset_index()  # Include opponent stats in the same row for machine learning model\n",
    "    game_opp.columns += \"_opp\"\n",
    "\n",
    "    full_game = pd.concat([dataframe, game_opp], axis=1)\n",
    "    full_game = full_game.drop(columns=['index_opp'])\n",
    "\n",
    "    # Add additional metadata:\n",
    "    full_game[\"Home\"] = [0,1] # Define home and away team\n",
    "    full_game[\"Won\"] = full_game[\"Score\"] > full_game[\"Score_opp\"] # Determine who won the game\n",
    "\n",
    "    full_game[\"Season\"] = read_season_info(soup) # Determines which season this game happened\n",
    "    \n",
    "    full_game[\"Date\"] = os.path.basename(box_score)[:8] # Gets date from file name\n",
    "    full_game[\"Date\"] = pd.to_datetime(full_game[\"Date\"], format=\"%Y%m%d\") # Converts and formats this to a datetime\n",
    "\n",
    "    # Add info:\n",
    "    # all columns to lowercase\n",
    "    return full_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2400f37a-7638-43bb-a8ee-8f8b6516bfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxscores: 212 / 212\n"
     ]
    }
   ],
   "source": [
    "# Loop to parse each HTML file\n",
    "games = []\n",
    "for box_score in box_scores:\n",
    "    soup = parse_html(box_score)\n",
    "    full_game = get_dataframe(soup, box_score)\n",
    "    games.append(full_game)\n",
    "    \n",
    "print(f\"Boxscores: {len(games)} / {len(box_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff52755c-db29-4334-a5c9-daaff9d07326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "games_dataframe = pd.concat(games, ignore_index=True)\n",
    "games_dataframe = games_dataframe.sort_values('Date') # Sort each game by date\n",
    "games_dataframe = games_dataframe.reset_index(drop=True) # Reset the indices\n",
    "games_dataframe.rename(columns={'TO': 'T_O'}, inplace=True) # TO is a keyword in SQL\n",
    "\n",
    "# Replace \"OAK\" with \"LVR\" in the \"Tm\" and \"Tm_opp\" columns\n",
    "games_dataframe['Tm'] = games_dataframe['Tm'].replace('OAK', 'LVR')\n",
    "games_dataframe['Tm_opp'] = games_dataframe['Tm_opp'].replace('OAK', 'LVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7067437c-51c4-4eb5-8e8a-dff9fb52abd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL database.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish a connection to the PostgreSQL database\n",
    "    connection = psycopg2.connect(\n",
    "        host = \"localhost\",\n",
    "        dbname = \"nfl\",\n",
    "        user = \"postgres\",\n",
    "        password = \"Plenoir2002!\", # Include correct password\n",
    "        port = 5432\n",
    "    )\n",
    "    print(\"Connected to PostgreSQL database.\")\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error connecting to PostgreSQL database: \", error)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0cb5b51-00aa-453f-88cd-a4770be2e9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the last gameid from the database\n",
    "cursor.execute(\"SELECT MAX(gameid) FROM {}\".format('nfl_data'))\n",
    "last_gameid = cursor.fetchone()[0]\n",
    "\n",
    "# Increment gameid for each row in your DataFrame\n",
    "games_dataframe['gameid'] = range(last_gameid + 1, last_gameid + 1 + len(games_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62340dc9-e5bf-48e5-b966-c38cf06180cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert data into the table\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO {} ({}) VALUES ({});\n",
    "\"\"\".format(\n",
    "    'nfl_data',\n",
    "    ', '.join(games_dataframe.columns.tolist()),  # Include all columns in the column list\n",
    "    ', '.join(['%s'] * len(games_dataframe.columns))  # Adjust the placeholders\n",
    ")\n",
    "\n",
    "# Convert DataFrame to a list of tuples\n",
    "data_values = [tuple(row) for row in games_dataframe.values]\n",
    "\n",
    "# Execute the insert query with data values\n",
    "for row in data_values:\n",
    "    cursor.execute(insert_query, row)\n",
    "    \n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79a4de-95d1-499e-94fb-c329c1257847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update results ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd933f9f-1500-41d5-8652-3c936040f4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Close the cursor and the database connection\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
